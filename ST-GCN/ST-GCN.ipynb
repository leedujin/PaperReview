{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"skeleton_data\\S001C001P001R001A001.skeleton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.8063e+00,  2.0895e+00,  5.9736e-01,  1.8155e+00,  3.0108e-01,\n",
       "         4.3935e-01,  3.3295e+00,  3.8801e-01, -3.0477e-02,  7.7179e-01,\n",
       "         2.1462e-01,  1.5370e-02, -3.6588e-01, -7.6570e-01,  9.8845e-01,\n",
       "         7.7133e-01,  3.6575e-01,  1.8198e-01, -4.3959e-01, -2.9773e-01,\n",
       "        -4.1035e-02,  1.9726e-01,  7.0137e-01,  1.8711e+00,  2.3458e+00,\n",
       "         6.1498e-01,  6.3956e-01,  1.1892e+00,  5.2219e-01,  3.0641e+00,\n",
       "         2.1344e+00,  2.0818e-03,  9.2607e-01,  2.0819e+00,  2.1478e+00,\n",
       "        -1.4458e-01,  1.1439e+00,  1.7609e+00,  1.2734e+00,  1.5568e+00,\n",
       "        -1.3236e-01,  2.4181e+00,  1.3456e+00, -2.6920e-01,  1.0540e+00,\n",
       "         2.8673e-01,  1.1979e+00,  2.3122e+00,  1.3570e+00, -2.6434e+00,\n",
       "         7.0119e-01,  2.1686e+00,  1.7480e+00, -1.7179e+00,  1.6029e+00,\n",
       "         2.3255e+00,  1.4212e+00,  9.8104e-01,  1.9362e+00,  5.2958e-01,\n",
       "         5.3748e-01,  1.3583e+00, -1.1335e+00,  2.2510e+00,  3.1502e-01,\n",
       "         2.5226e+00,  1.6503e+00, -3.2971e-01,  2.5876e+00,  1.8445e+00,\n",
       "         1.4361e+00, -8.0421e-01,  1.2427e+00,  9.7552e-01,  8.6706e-01,\n",
       "         1.7860e+00,  3.3992e-01,  2.2655e+00,  3.4578e-02,  1.8199e+00,\n",
       "         4.3753e-01,  2.5331e+00,  1.7043e+00,  1.9918e-01,  1.9744e+00,\n",
       "         1.5501e+00,  2.8520e-01, -2.1258e-02,  1.1361e+00,  2.1550e-02,\n",
       "        -1.4486e+00, -4.7733e-01, -9.3841e-01,  7.9083e-01,  2.0135e+00,\n",
       "         9.5993e-01, -5.4588e-01,  4.7578e-01,  2.6870e+00,  1.4139e+00,\n",
       "         2.8964e+00, -6.8921e-01,  1.1492e+00, -5.9631e-02,  5.6079e-01,\n",
       "         6.0244e-01, -2.6209e-01,  4.0413e-01,  2.6071e+00, -4.7480e-01,\n",
       "         5.7828e-01,  1.9696e-02,  2.1037e+00,  1.1224e+00,  1.3861e+00,\n",
       "         3.0420e+00, -5.6709e-01, -6.2357e-02,  1.1463e+00,  5.6834e-01,\n",
       "         3.0917e+00,  6.9681e-01,  1.7277e+00,  9.3775e-01, -6.0594e-01,\n",
       "         3.4124e+00,  1.7736e+00,  6.8871e-02,  2.4418e-01,  1.6927e+00,\n",
       "         1.8307e+00,  6.1977e-02,  1.3469e+00,  2.3344e+00,  1.1303e+00,\n",
       "         3.0051e+00,  2.0419e+00, -8.1388e-01,  3.1299e-01,  3.4167e-03,\n",
       "        -4.0216e-01,  9.8586e-01,  6.2821e-01,  2.9762e-01, -7.2544e-01,\n",
       "         2.7255e+00,  2.5668e+00,  1.0146e+00,  5.8216e-01,  1.3555e+00,\n",
       "        -4.3721e-01,  9.9304e-01,  1.1130e-01, -3.6780e-01,  5.2960e-01,\n",
       "         2.5341e+00,  2.2206e+00,  1.0795e+00,  3.7905e-03,  3.8327e-02,\n",
       "         2.5526e+00,  1.5085e+00,  1.6848e+00,  1.8688e+00,  1.3439e+00,\n",
       "         1.2819e+00, -2.1328e-01,  1.0915e+00, -1.0053e+00,  2.0947e+00,\n",
       "         8.4723e-02,  2.5435e+00,  1.5210e+00,  1.0025e+00,  4.5465e-01,\n",
       "         8.6836e-01,  1.5467e+00,  1.1586e+00,  1.3499e+00,  2.2797e+00,\n",
       "        -2.4170e-01,  1.7045e+00, -1.9098e-01,  6.6775e-01, -5.1287e-01,\n",
       "         1.3379e-01, -3.8940e-01,  8.3505e-01,  1.4611e+00,  5.8061e-02,\n",
       "         4.7097e-01,  1.9289e+00,  1.6810e+00,  2.2108e+00,  3.0480e-01,\n",
       "         1.2364e+00,  2.3550e+00,  1.2280e+00,  1.1790e+00, -4.7827e-01,\n",
       "         2.1955e+00, -2.4021e-01, -1.5024e-01,  2.0175e+00,  2.1273e+00,\n",
       "         2.9640e-01,  1.0205e+00,  9.4220e-01,  1.1365e+00,  1.4156e+00,\n",
       "         7.9638e-01,  1.7328e+00,  2.3857e+00,  3.2080e-01,  9.6181e-02,\n",
       "         5.7955e-01,  2.4159e+00,  9.7154e-01,  4.0424e-01,  5.9085e-01,\n",
       "         9.4569e-01, -1.6104e-01,  2.1745e+00,  2.7941e+00,  2.1109e+00,\n",
       "         6.1655e-01,  1.4966e+00, -1.7668e+00,  1.6206e+00,  1.0056e+00,\n",
       "         6.7162e-01, -1.2736e-01, -1.6117e+00, -9.4677e-01,  2.2470e+00,\n",
       "         7.4236e-01,  9.0791e-01, -8.8286e-02, -1.0084e+00,  1.9093e+00,\n",
       "         1.4850e+00,  1.0501e+00,  7.1433e-01, -5.7352e-02,  1.3582e+00,\n",
       "         2.0402e-01,  2.4214e+00,  1.4990e+00,  1.7529e+00, -5.7968e-01,\n",
       "        -2.8465e-01,  1.9339e+00,  1.0848e+00,  9.6785e-01, -1.0984e+00,\n",
       "         1.9276e-01])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "mu = torch.tensor()\n",
    "logvar = torch.tensor()\n",
    "\n",
    "std = logvar.mul(0.1).exp()\n",
    "print(std.shape)\n",
    "std = torch.clamp(std, max = 100)\n",
    "print(std.shape)\n",
    "eps = torch.empty_like(std).normal_()\n",
    "print(eps.shape)\n",
    "\n",
    "eps.mul(std) + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3362, 0.2039, 0.1845, 0.2753]])\n",
      "torch.Size([4, 1])\n",
      "tensor([[-0.2039, -0.3362, -0.3362, -0.3362]])\n",
      "tensor([[-0.2039, -0.3362, -0.3362, -0.3362]])\n",
      "tensor([-0.2500])\n",
      "tensor([[-0.2085, -0.3276, -0.3276, -0.3276]])\n",
      "tensor([[-0.2039, -0.3362, -0.3362, -0.3362]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-d9f5ce4792ad>:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logprobs = F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[0.7, 0.2, 0.1, 0.5]])\n",
    "target = torch.tensor([1, 0, 0, 0])\n",
    "confidence = 0.9\n",
    "logprobs = F.softmax(x)\n",
    "print(logprobs)\n",
    "print(target.unsqueeze(1).shape)\n",
    "nll_loss = -logprobs.gather(dim = -1, index = target.unsqueeze(0))\n",
    "print(nll_loss)\n",
    "nll_loss = nll_loss.squeeze(1)\n",
    "print(nll_loss)\n",
    "smooth_loss = -logprobs.mean(dim = -1)\n",
    "print(smooth_loss)\n",
    "loss = confidence * nll_loss + 0.1 * smooth_loss\n",
    "print(loss)\n",
    "\n",
    "print(nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d4\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\d4\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\d4\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTURGB+D 120 skeleton files detected, will generate both ntu60_3danno.pkl and ntu120_3danno.pkl. \n"
     ]
    }
   ],
   "source": [
    "import copy as cp\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "from mmcv import dump\n",
    "from tqdm import tqdm\n",
    "\n",
    "from smp import mrlines\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "def parse_skeleton_file(ske_name, root = \"skeleton_data\"):\n",
    "    ske_file = osp.join(root, ske_name + \".skeleton\")\n",
    "\n",
    "    lines = mrlines(ske_file)\n",
    "    idx = 0\n",
    "    num_frames = int(lines[0])\n",
    "    num_joints = 25\n",
    "    idx += 1\n",
    "\n",
    "    body_data = dict()\n",
    "    fidx = 0\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        num_bodies = int(lines[idx])\n",
    "        idx += 1\n",
    "        if num_bodies == 0:\n",
    "            continue\n",
    "        for b in range(num_bodies):\n",
    "            bodyID = int(lines[idx].split()[0])\n",
    "            if bodyID not in body_data:\n",
    "                kpt = []\n",
    "                body_data[bodyID] = dict(kpt = kpt, start = fidx)\n",
    "            idx += 1\n",
    "            assert int(lines[idx]) == 25\n",
    "            idx += 1\n",
    "            joints = np.zeros((25, 3), dtype = np.float32)\n",
    "\n",
    "            for j in range(num_joints):\n",
    "                line = lines[idx].split()\n",
    "                joints[j, :3] = np.array(line[:3], dtype=np.float32)\n",
    "                idx += 1\n",
    "            body_data[bodyID]['kpt'].append(joints)\n",
    "        fidx += 1\n",
    "\n",
    "    for k in body_data:\n",
    "        body_data[k]['motion'] = np.sum(np.var(np.vstack(body_data[k]['kpt']), axis=0))\n",
    "        body_data[k]['kpt'] = np.stack(body_data[k]['kpt'])\n",
    "\n",
    "    assert idx == len(lines)\n",
    "    return body_data\n",
    "\n",
    "\n",
    "def spread_denoising(body_data_list):\n",
    "    wh_ratio = 0.8\n",
    "    spnoise_ratio = 0.69754\n",
    "\n",
    "    def get_valid_frames(kpt):\n",
    "        valid_frames = []\n",
    "        for i in range(kpt.shape[0]):\n",
    "            x, y = kpt[i, :, 0], kpt[i, :, 1]\n",
    "            if (x.max() - x.min()) <= wh_ratio * (y.max() - y.min()):\n",
    "                valid_frames.append(i)\n",
    "        return valid_frames\n",
    "\n",
    "    for item in body_data_list:\n",
    "        valid_frames = get_valid_frames(item['kpt'])\n",
    "        if len(valid_frames) == item['kpt'].shape[0]:\n",
    "            item['flag'] = True\n",
    "            continue\n",
    "        ratio = len(valid_frames) / item['kpt'].shape[0]\n",
    "        if 1 - ratio >= spnoise_ratio:\n",
    "            item['flag'] = False\n",
    "        else:\n",
    "            item['flag'] = True\n",
    "            item['motion'] = min(item['motion'],\n",
    "                                 np.sum(np.var(item['kpt'][valid_frames].reshape(-1, 3), axis=0)))\n",
    "    body_data_list = [item for item in body_data_list if item['flag']]\n",
    "    assert len(body_data_list) >= 1\n",
    "    _ = [item.pop('flag') for item in body_data_list]\n",
    "    body_data_list.sort(key=lambda x: -x['motion'])\n",
    "    return body_data_list\n",
    "\n",
    "\n",
    "def non_zero(kpt):\n",
    "    s = 0\n",
    "    e = kpt.shape[1]\n",
    "    while np.sum(np.abs(kpt[:, s])) < eps:\n",
    "        s += 1\n",
    "    while np.sum(np.abs(kpt[:, e - 1])) < eps:\n",
    "        e -= 1\n",
    "    return kpt[:, s: e]\n",
    "\n",
    "\n",
    "def gen_keypoint_array(body_data):\n",
    "    length_threshold = 11\n",
    "\n",
    "    body_data = cp.deepcopy(list(body_data.values()))\n",
    "    body_data.sort(key=lambda x: -x['motion'])\n",
    "    if len(body_data) == 1:\n",
    "        return body_data[0]['kpt'][None]\n",
    "    else:\n",
    "        body_data = [item for item in body_data if item['kpt'].shape[0] > length_threshold]\n",
    "        if len(body_data) == 1:\n",
    "            return body_data[0]['kpt'][None]\n",
    "        body_data = spread_denoising(body_data)\n",
    "        if len(body_data) == 1:\n",
    "            return body_data[0]['kpt'][None]\n",
    "        max_fidx = 0\n",
    "\n",
    "        for item in body_data:\n",
    "            max_fidx = max(max_fidx, item['start'] + item['kpt'].shape[0])\n",
    "        keypoint = np.zeros((2, max_fidx, 25, 3), np.float32)\n",
    "\n",
    "        s1, e1, s2, e2 = body_data[0]['start'], body_data[0]['start'] + body_data[0]['kpt'].shape[0], 0, 0\n",
    "        keypoint[0, s1: e1] = body_data[0]['kpt']\n",
    "        for item in body_data[1:]:\n",
    "            s, e = item['start'], item['start'] + item['kpt'].shape[0]\n",
    "            if max(s1, s) >= min(e1, e):\n",
    "                keypoint[0, s: e] = item['kpt']\n",
    "                s1, e1 = min(s, s1), max(e, e1)\n",
    "            elif max(s2, s) >= min(e2, e):\n",
    "                keypoint[1, s: e] = item['kpt']\n",
    "                s2, e2 = min(s, s2), max(e, e2)\n",
    "\n",
    "        keypoint = non_zero(keypoint)\n",
    "        if np.sum(np.abs(keypoint[0, 0, 1])) < eps and np.sum(np.abs(keypoint[1, 0, 1])) > eps:\n",
    "            keypoint = keypoint[::-1]\n",
    "        return keypoint\n",
    "\n",
    "\n",
    "root = 'skeleton_data'\n",
    "skeleton_files = os.listdir(root)\n",
    "names = [x.split('.')[0] for x in skeleton_files]\n",
    "names.sort()\n",
    "missing = mrlines('ntu120_missing.txt')\n",
    "missing = set(missing)\n",
    "names = [x for x in names if x not in missing]\n",
    "\n",
    "extended = False\n",
    "for name in names:\n",
    "    if int(name.split('A')[-1]) > 60:\n",
    "        extended = True\n",
    "        print('NTURGB+D 120 skeleton files detected, will generate both ntu60_3danno.pkl and ntu120_3danno.pkl. ')\n",
    "        break\n",
    "\n",
    "if not extended:\n",
    "    print('NTURGB+D 120 skeleton files not detected, will only generate ntu60_3danno.pkl. ')\n",
    "\n",
    "\n",
    "def gen_anno(name):\n",
    "    body_data = parse_skeleton_file(name, root)\n",
    "    if len(body_data) == 0:\n",
    "        return None\n",
    "    keypoint = gen_keypoint_array(body_data).astype(np.float16)\n",
    "    label = int(name.split('A')[-1]) - 1\n",
    "    total_frames = keypoint.shape[1]\n",
    "    return dict(frame_dir=name, label=label, keypoint=keypoint, total_frames=total_frames)\n",
    "\n",
    "\n",
    "anno_dict = {}\n",
    "num_process = 8\n",
    "\n",
    "if num_process == 1:\n",
    "    # Each annotations has 4 keys: frame_dir, label, keypoint, total_frames\n",
    "    for name in tqdm(names):\n",
    "        anno_dict[name] = gen_anno(name)\n",
    "else:\n",
    "    pool = mp.Pool(num_process)\n",
    "    annotations = pool.map(gen_anno, names)\n",
    "    pool.close()\n",
    "    for anno in annotations:\n",
    "        anno_dict[anno['frame_dir']] = anno\n",
    "\n",
    "names = [x for x in names if anno_dict is not None]\n",
    "training_subjects = [\n",
    "    1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35,\n",
    "    38, 45, 46, 47, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 70, 74, 78,\n",
    "    80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 94, 95, 97, 98, 100, 103\n",
    "]\n",
    "\n",
    "if extended:\n",
    "    xsub_train = [name for name in names if int(name.split('P')[1][:3]) in training_subjects]\n",
    "    xsub_val = [name for name in names if int(name.split('P')[1][:3]) not in training_subjects]\n",
    "    xset_train = [name for name in names if int(name.split('S')[1][:3]) % 2 == 0]\n",
    "    xset_val = [name for name in names if int(name.split('S')[1][:3]) % 2 == 1]\n",
    "    split = dict(xsub_train=xsub_train, xsub_val=xsub_val, xset_train=xset_train, xset_val=xset_val)\n",
    "    annotations = [anno_dict[name] for name in names]\n",
    "    dump(dict(split=split, annotations=annotations), 'ntu120_3danno.pkl')\n",
    "\n",
    "names = [name for name in names if int(name.split('A')[-1]) <= 60]\n",
    "xsub_train = [name for name in names if int(name.split('P')[1][:3]) in training_subjects]\n",
    "xsub_val = [name for name in names if int(name.split('P')[1][:3]) not in training_subjects]\n",
    "xview_train = [name for name in names if 'C001' not in name]\n",
    "xview_val = [name for name in names if 'C001' in name]\n",
    "split = dict(xsub_train=xsub_train, xsub_val=xsub_val, xview_train=xview_train, xview_val=xview_val)\n",
    "annotations = [anno_dict[name] for name in names]\n",
    "dump(dict(split=split, annotations=annotations), 'ntu60_3danno.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph, Adjacency Matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Graph(): \n",
    "    # for NTU RGB+D, spatial strategy\n",
    "\n",
    "    def __init__(self, max_hop = 1): # distance의 개수에 따라 adjacency matrix의 개수도 달라짐\n",
    "        self.max_hop = max_hop # the number of distance between two edges which is I pay attention to look at\n",
    "        self_loop = [(i, i) for i in range(25)] # self loop\n",
    "\n",
    "        neighbor_list = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n",
    "                        (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n",
    "                        (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n",
    "                        (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n",
    "                        (22, 23), (23, 8), (24, 25), (25, 12)]\n",
    "                        # connected edges list\n",
    "\n",
    "        neighbor_list = [(i - 1, j - 1) for (i, j) in neighbor_list] # for indexing\n",
    "        self.edge = self_loop + neighbor_list\n",
    "        self.center = 21 - 1\n",
    "\n",
    "        dis = np.zeros((25, 25))\n",
    "\n",
    "        for i, j in self.edge:\n",
    "            dis[i, j] = 1\n",
    "            dis[j, i] = 1\n",
    "\n",
    "        dis = np.stack([np.linalg.matrix_power(dis, d) for d in range(0, max_hop + 1)]) > 0\n",
    "        # 총 3개의 dis\n",
    "        mat = np.zeros((25, 25)) + np.inf\n",
    "        # \n",
    "        for i in range(max_hop, -1, -1):\n",
    "            mat[dis[i]] = i\n",
    "\n",
    "        self.dis = mat\n",
    "        self.get_adjacency_matrix()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A\n",
    "\n",
    "    def get_adjacency_matrix(self):\n",
    "        valid_distance = range(self.max_hop + 1) # (0, 1, ..., max_hop)\n",
    "        adjacency = np.zeros((25, 25))\n",
    "\n",
    "        for dis in valid_distance:\n",
    "            adjacency[self.dis == dis] = 1\n",
    "            \n",
    "        normalize_adjacency = normalize_digraph(adjacency)   \n",
    "\n",
    "        A = []\n",
    "        for dis in valid_distance:\n",
    "\n",
    "            A_root = np.zeros((25, 25)) # adjacency matrix of root\n",
    "            A_closer = np.zeros((25, 25)) # adjacency matrix of closer\n",
    "            A_further = np.zeros((25, 25)) # adjacency matrix of further\n",
    "            \n",
    "            for i in range(25):\n",
    "                for j in range(25):\n",
    "                    if self.dis[j, i] == dis:\n",
    "                        if self.dis[j, self.center] == self.dis[i, self.center]: # self.center = 1\n",
    "                            A_root[j, i] = normalize_adjacency[j, i]\n",
    "                        elif self.dis[j, self.center] > self.dis[i, self.center]:\n",
    "                            A_closer[j, i] = normalize_adjacency[j, i]\n",
    "                        else :\n",
    "                            A_further[j, i] = normalize_adjacency[j, i]\n",
    "            \n",
    "            if dis == 0:\n",
    "                A.append(A_root)\n",
    "                \n",
    "            else :\n",
    "                A.append(A_root + A_closer)\n",
    "                A.append(A_further)\n",
    "\n",
    "        A = np.stack(A)\n",
    "        self.A = A\n",
    "\n",
    "def normalize_digraph(A):\n",
    "    Dl = np.sum(A, 0) #\n",
    "    for i in range(len(Dl)):\n",
    "        for j in range(len(Dl)):\n",
    "            if Dl[i] > 0:\n",
    "                A[j, i] = A[j, i] / Dl[i]\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 25, 25)\n"
     ]
    }
   ],
   "source": [
    "graph = Graph(2)\n",
    "print(graph.A.shape) # a_root, a_closer1, a_further1, a_closer2, a_further2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, graph):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.graph = graph\n",
    "        A = torch.tensor(self.graph.A, dtype = torch.float32, requires_grad = False)\n",
    "        self.register_buffer(\"A\", A)\n",
    "\n",
    "        spatial_kernel_size = A.size(0)\n",
    "        temporal_kernel_size = 9\n",
    "        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n",
    "        self.data_bn = nn.BatchNorm1d(54)\n",
    "\n",
    "        self.st_gcn_networks = nn.ModuleList((\n",
    "            st_gcn(3, 64, kernel_size, 1, residual = False), # 1\n",
    "            st_gcn(64, 64, kernel_size, 1), # 2\n",
    "            st_gcn(64, 64, kernel_size, 1), # 3\n",
    "            st_gcn(64, 64, kernel_size, 1), # 4\n",
    "            st_gcn(64, 128, kernel_size, 2), # 5\n",
    "            st_gcn(128, 128, kernel_size, 1), # 6\n",
    "            st_gcn(128, 128, kernel_size, 1), # 7\n",
    "            st_gcn(128, 256, kernel_size, 2), # 8\n",
    "            st_gcn(256, 256, kernel_size, 1), # 9\n",
    "            st_gcn(256, 256, kernel_size, 1), # 10\n",
    "        ))\n",
    "\n",
    "        self.edge_importance = nn.ParameterList([\n",
    "            nn.Parameter(torch.ones(self.A.size())) for i in self.st_gcn_networks\n",
    "        ])\n",
    "\n",
    "        self.fcn = nn.Conv2d(256, 120, kernel_size = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, T, V, M = x.size()\n",
    "        x = x.permute(0, 4, 3, 1, 2).contiguous()\n",
    "        x = x.view(B * M, V * C, T)\n",
    "        x = self.data_bn(x)\n",
    "        x = x.view(B, M, V, C, T)\n",
    "        x = x.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        x = x.view(B * M, C, T, V)\n",
    "\n",
    "        for graphconv, importance in zip(self.st_gcn_networks, self.edge_importance):\n",
    "            x, A = graphconv(x, self.A * importance)\n",
    "\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "\n",
    "        x = x.view(B, M, -1, 1, 1).mean(dim = 1)\n",
    "        x = self.fcn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class st_gcn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, residual = True, dropout = 0):\n",
    "        super(st_gcn, self).__init__()\n",
    "        self.gcn_kernel_size = kernel_size[1]\n",
    "        self.tcn_kernel_size = kernel_size[0]\n",
    "        self.gcn = nn.Conv2d(in_channels, out_channels * self.gcn_kernel_size, 1)\n",
    "\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(out_channels, out_channels, (self.tcn_kernel_size, 1), padding = ((self.tcn_kernel_size - 1) // 2, 0)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout(dropout, inplace = True))\n",
    "\n",
    "        if residual != True:\n",
    "            self.residual = lambda x : 0\n",
    "        elif in_channels == out_channels :\n",
    "            self.residual = lambda x : x\n",
    "        else :\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=(1,1), stride = (stride, 1)),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            ) \n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        res = self.residual(x) \n",
    "\n",
    "        x = self.convnet(x)\n",
    "        n, kc, t, v = x.size()\n",
    "        x = x.view(n, self.kernel_size, kc // self.kernel_size, t, v) # 5 dim\n",
    "        x = torch.einsum(\"nkctv, kvw -> nctw\", (x, A))\n",
    "\n",
    "        x = self.tcn(x) + res\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        return x, A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "062945cf05476a4a5aad44300c5bd9ba606571b08acbd052637fc40425c6d69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
